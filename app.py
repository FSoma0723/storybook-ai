import streamlit as st
from ai_init import initialize_ai
from chat_manager import get_ai_response, get_current_persona_and_situation_description
from tts_handler import synthesize_speech_with_gemini_to_wav
from PIL import Image
import io
import os
import tempfile
import whisper
from audio_recorder_streamlit import audio_recorder
import ast

# --- AIã®åˆæœŸåŒ– ---
if "ai_initialized" not in st.session_state:
    st.session_state.ai_initialized = False

if not st.session_state.ai_initialized:
    if initialize_ai():
        st.session_state.ai_initialized = True
    else:
        st.error("AIã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç’°å¢ƒå¤‰æ•° 'GEMINI_API_KEY' ã‚„APIã‚­ãƒ¼ã®æœ‰åŠ¹æ€§ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()

# --- Streamlitã‚¢ãƒ—ãƒªã®UIè¨­å®š ---
st.title("çµµæœ¬ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼AIãƒãƒ£ãƒƒãƒˆ")
st.caption("ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€AIãŒãã®çµµã®ã€Œäººé–“ã€ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã«ãªã‚Šãã‚Šã€çŠ¶æ³ã‚‚ç†è§£ã—ã¦ãŠè©±ã—ã—ã¾ã™ã€‚")

if "intro_played" not in st.session_state:
    st.session_state.intro_played = False

if not st.session_state.intro_played:
    with open("narration/intro_narration.wav", "rb") as f:
        audio_bytes = f.read()
    st.audio(audio_bytes, format="audio/wav", autoplay=True)
    st.session_state.intro_played = True

# --- ä¼šè©±å±¥æ­´ã®ç®¡ç† ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- é€ä¿¡å¾…ã¡ã®ç”»åƒï¼ˆPillow Imageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰ã®çŠ¶æ…‹ç®¡ç† ---
if "image_to_process_on_send" not in st.session_state:
    st.session_state.image_to_process_on_send = None 
if "uploaded_file_name" not in st.session_state:
    st.session_state.uploaded_file_name = None
if "uploader_key_suffix" not in st.session_state:
    st.session_state.uploader_key_suffix = 0
if "camera_key_suffix" not in st.session_state:
    st.session_state.camera_key_suffix = 0

# --- ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒªã‚¢ ---
# --- ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ or ã‚«ãƒ¡ãƒ©æ’®å½±ã‚¨ãƒªã‚¢ ---
with st.container():
    st.markdown("##### ã‚¹ãƒ†ãƒƒãƒ—1: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã«ãªã£ã¦ã»ã—ã„ã€Œäººé–“ã€ã¨ã€Œãã®çŠ¶æ³ã€ãŒæã‹ã‚ŒãŸçµµæœ¬ã®ãƒšãƒ¼ã‚¸ã‚’é¸æŠ")
    
    uploaded_file_obj = st.file_uploader("ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠï¼ˆä»»æ„ï¼‰", type=["jpg", "jpeg", "png"],key=f"file_uploader_{st.session_state.get('uploader_key_suffix', 0)}"
)
    camera_image_obj = st.camera_input("ã¾ãŸã¯ã‚«ãƒ¡ãƒ©ã§æ’®å½±",
    key=f"camera_input_{st.session_state.camera_key_suffix}")

    pil_image = None
    file_name = None

    if uploaded_file_obj is not None:
        try:
            image_bytes = uploaded_file_obj.getvalue()
            pil_image = Image.open(io.BytesIO(image_bytes))
            file_name = uploaded_file_obj.name
            
            if "mic_guide_played" not in st.session_state:
                with open("narration/mic_guide_narration.wav", "rb") as f:
                    audio_bytes = f.read()
                st.audio(audio_bytes, format="audio/wav", autoplay=True)
                st.session_state.mic_guide_played = True
                
        except Exception as e:
            st.error(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    elif camera_image_obj is not None:
        try:
            image_bytes = camera_image_obj.getvalue()
            pil_image = Image.open(io.BytesIO(image_bytes))
            file_name = "captured_from_camera.jpg"
            
            if "mic_guide_played" not in st.session_state:
                with open("narration/mic_guide_narration.wav", "rb") as f:
                    audio_bytes = f.read()
                st.audio(audio_bytes, format="audio/wav", autoplay=True)
                st.session_state.mic_guide_played = True
        except Exception as e:
            st.error(f"ã‚«ãƒ¡ãƒ©ç”»åƒã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    if pil_image:
        st.session_state.image_to_process_on_send = pil_image
        st.session_state.uploaded_file_name = file_name
        st.success(f"ç”»åƒã€Œ{file_name}ã€ãŒé¸æŠã•ã‚Œã¾ã—ãŸã€‚ä¸‹ã®ãƒãƒ£ãƒƒãƒˆæ¬„ã‹ã‚‰è©±ã—ã‹ã‘ã¦ã¿ã¾ã—ã‚‡ã†ï¼")

        col1_img, col2_btn = st.columns([0.8, 0.2])
        with col1_img:
            st.image(pil_image, caption=f"é¸æŠä¸­ã®ç”»åƒ: {file_name}", width=200)
        with col2_btn:
            if st.button("ã“ã®ç”»åƒã‚’ã‚¯ãƒªã‚¢", key="clear_image_button_main"):
                st.session_state.image_to_process_on_send = None
                st.session_state.uploaded_file_name = None
                st.session_state.uploader_key_suffix = st.session_state.get('uploader_key_suffix', 0) + 1
                st.session_state.camera_key_suffix = st.session_state.camera_key_suffix + 1
                st.rerun()

st.divider()

# --- ç¾åœ¨ã®AIã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ»çŠ¶æ³è¡¨ç¤º ---
current_persona_info_for_ui = get_current_persona_and_situation_description()
if current_persona_info_for_ui:
    lines = current_persona_info_for_ui.splitlines()
    display_text = lines[0] 
    situation_summary = ""
    for line in lines:
        if "å ´æ‰€ï¼š" in line or "é›°å›²æ°—ï¼š" in line: 
            situation_summary += f" ({line.split('ï¼š', 1)[1].strip()})" if 'ï¼š' in line else f" ({line.strip()})"
            break 
    
    if "ã‚¨ãƒ©ãƒ¼ï¼š" in display_text or "ï¼ˆ" in display_text: 
        st.info(f"AIã®ç¾åœ¨ã®çŠ¶æ…‹: {display_text}")
    else:
        char_name_part = display_text.split("åå‰ï¼š",1)[-1].splitlines()[0].strip() if "åå‰ï¼š" in display_text else display_text.strip()
        st.info(f"AIã¯ç¾åœ¨ã€Œ{char_name_part}ã€ã¨ã—ã¦å¿œç­”ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ã€‚{situation_summary}")


# --- â˜…ä¿®æ­£ç‚¹â‘¡: ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´ã‚’ç”»é¢ã«è¡¨ç¤ºï¼ˆéŸ³å£°ã‚‚å«ã‚€ï¼‰ ---
st.subheader("ã‚¹ãƒ†ãƒƒãƒ—2: ä¼šè©±ã—ã¦ã¿ã‚ˆã†ï¼")
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        if message.get("image_data_for_ui"): 
            st.image(message["image_data_for_ui"], width=200, caption="ã‚ãªãŸãŒé€ä¿¡ã—ãŸç”»åƒ")
        if message.get("text_content"):
            st.markdown(message["text_content"])
        # â˜…è¿½åŠ : ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°å†ç”Ÿãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º
        if message.get("audio_data"):
            st.audio(message["audio_data"], format="audio/wav", autoplay=True)
st.markdown("---")


# --- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¥åŠ›ã‚’å—ã‘ä»˜ã‘ ---
user_input_text = st.chat_input("ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã«è©±ã—ã‹ã‘ã¦ã¿ã‚ˆã†...", key="chat_input_main_text")

if user_input_text:
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
    user_message_data_for_ui = {"role": "user", "text_content": user_input_text}
    image_to_send_to_ai = st.session_state.get("image_to_process_on_send", None)
    
    if image_to_send_to_ai:
        user_message_data_for_ui["image_data_for_ui"] = image_to_send_to_ai
    
    st.session_state.messages.append(user_message_data_for_ui)
    
    # AIã®å¿œç­”ã‚’å–å¾—
    ai_response_text = get_ai_response(user_prompt=user_input_text, image_data=image_to_send_to_ai)

    # --- â˜…ä¿®æ­£ç‚¹â‘ : AIã®å¿œç­”ã¨éŸ³å£°ã‚’å±¥æ­´ã«è¿½åŠ  ---
    assistant_message_data = {"role": "assistant", "text_content": ai_response_text}
  
    # éŸ³å£°ç”Ÿæˆã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    wav_path = synthesize_speech_with_gemini_to_wav(ai_response_text, filename="latest_output.wav")
    
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ç”Ÿæˆã•ã‚ŒãŸã‚‰ã€ãã®ä¸­èº«ã‚’èª­ã¿è¾¼ã‚“ã§ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
    if wav_path and os.path.exists(wav_path):
        with open(wav_path, "rb") as f:
            audio_bytes = f.read()
        assistant_message_data["audio_data"] = audio_bytes

    # ãƒ†ã‚­ã‚¹ãƒˆã¨éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€AIã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
    st.session_state.messages.append(assistant_message_data)
    
    # ã“ã®ã‚¿ãƒ¼ãƒ³ã§ãƒšãƒ«ã‚½ãƒŠè¨­å®šã«ç”»åƒã‚’ä½¿ã£ãŸå ´åˆã€æ¬¡ã®ã‚¿ãƒ¼ãƒ³ã§ã¯ç”»åƒãªã—ã§ä¼šè©±ã‚’ç¶šã‘ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ã‚¯ãƒªã‚¢
    if image_to_send_to_ai:
        st.session_state.image_to_process_on_send = None
        st.session_state.uploaded_file_name = None
        st.session_state.uploader_key_suffix = st.session_state.get('uploader_key_suffix', 0) + 1
        st.session_state.camera_key_suffix = st.session_state.camera_key_suffix + 1
    st.rerun()


# --- ğŸ¤ éŸ³å£°å…¥åŠ›ã§ä¼šè©±ã™ã‚‹ã‚¨ãƒªã‚¢ ---
# --- ğŸ¤ ãƒã‚¤ã‚¯ã§è©±ã—ã‹ã‘ã‚‹ã‚¨ãƒªã‚¢ ---
st.markdown("#### ğŸ¤ ãƒã‚¤ã‚¯ã§è©±ã—ã‹ã‘ã‚‹")
# ãƒã‚¤ã‚¯éŒ²éŸ³ãƒœã‚¿ãƒ³ï¼ˆWAVãƒ‡ãƒ¼ã‚¿ãŒè¿”ã‚‹ï¼‰
audio_bytes = audio_recorder()
if st.button("Save Recording"):
    if audio_bytes is None:
        st.error("ã¾ã éŒ²éŸ³ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    else:
        with open("recorded_audio.wav", "wb") as f:
            f.write(audio_bytes)
        st.success("Recording saved!")
    model = whisper.load_model("small")
    result = model.transcribe("recorded_audio.wav")
    voice_input_text=(result["text"])

    # ğŸ‘‡ ã“ã“ã‹ã‚‰ã¯ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã¨åŒã˜å‡¦ç†
    user_message_data_for_ui = {"role": "user", "text_content": voice_input_text}
    image_to_send_to_ai = st.session_state.get("image_to_process_on_send", None)

    if image_to_send_to_ai:
        user_message_data_for_ui["image_data_for_ui"] = image_to_send_to_ai

    st.session_state.messages.append(user_message_data_for_ui)

    ai_response_text = get_ai_response(user_prompt=voice_input_text, image_data=image_to_send_to_ai)

    assistant_message_data = {"role": "assistant", "text_content": ai_response_text}

    wav_path = synthesize_speech_with_gemini_to_wav(ai_response_text, filename="latest_output.wav")
    if wav_path and os.path.exists(wav_path):
        with open(wav_path, "rb") as f:
            audio_bytes = f.read()
        assistant_message_data["audio_data"] = audio_bytes
        

    st.session_state.messages.append(assistant_message_data)

    if image_to_send_to_ai:
        st.session_state.image_to_process_on_send = None
        st.session_state.uploaded_file_name = None
        st.session_state.uploader_key_suffix = st.session_state.get('uploader_key_suffix', 0) + 1
        st.session_state.camera_key_suffix = st.session_state.camera_key_suffix + 1
    st.rerun()
